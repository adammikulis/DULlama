This is the first iteration of a bare-bones local RAG pipeline in C# (requires .NET 8.0 runtime) using the [LLamaSharp](https://github.com/SciSharp/LLamaSharp) library. The program constructs a basic vector database consisting of the original text and the embeddings generated by the chosen LLM. The user submits a query, which gets matched against the db using cosine similarity. This returns the top results (original text) which are then integrated into the LLM's response. This has been tested and immediately corrects wrong information generated by an LLM by itself without RAG.

All Llama2 and Mistral 7B models at higher quantization (q) require a mininum of 16GB of RAM for CPU inference (8GB VRAM for a GPU if using the CUDA backend). Using models with smaller bit quantization takes less memory/processing at the cost of accuracy (the smallest models can run on an GTX 1060). This program has not been tested with 13B, 34B, 70B, or Mixtral variants but should be compatible if you have hardware that can support larger models. The path in Program.cs goes to a general "C:\ai\models" folder but you can change that to where you keep your model downloads.

You can configure the code to use a GPU by uncommenting the relevant line in the modelparams. Add one of the nuget packages listed here using Nuget Package Manager to use an Nvidia GPU (you may have to remove LLamaSharp.Backend.CPU):

https://www.nuget.org/packages/LLamaSharp.Backend.Cuda11

https://www.nuget.org/packages/LLamaSharp.Backend.Cuda12


I currently recommend Mistralv0.2 over Llama2 due to its faster processing time, smaller memory footprint, and superior benchmark performance. Choose a q that matches your system requirements and desired model performance (higher bits give better results at a cost of speed and higher memory footprint).

Mistral 7B downloads:

Math-enhanced: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-DARE-GGUF

Coding: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-code-ft-GGUF

General instruct: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF 


Llama2 7B downloads:

Coding: https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF

General: https://huggingface.co/TheBloke/Llama-2-7B-GGUF


Future goals:
- Add support for Microsoft's Phi-2
- Add processing for additional files/datatypes (sqr, sql, pdf, etc)
- Allow the LLM to process the entire conversation while still focusing on the most recent prompt to avoid repeating answers
- Integrate some telemetry like tok/s
- Build a better UI
